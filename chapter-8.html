<!DOCTYPE html><html><head><meta charset="utf-8"><style>body {
  width: 45em;
  border: 1px solid #ddd;
  outline: 1300px solid #fff;
  margin: 16px auto;
}

body .markdown-body
{
  padding: 30px;
}

@font-face {
  font-family: fontawesome-mini;
  src: url(data:font/woff;charset=utf-8;base64,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) format('woff');
}

@font-face {
  font-family: octicons-anchor;
  src: url(data:font/woff;charset=utf-8;base64,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) format('woff');
}

.markdown-body {
  font-family: sans-serif;
  -ms-text-size-adjust: 100%;
  -webkit-text-size-adjust: 100%;
  color: #333333;
  overflow: hidden;
  font-family: "Helvetica Neue", Helvetica, "Segoe UI", Arial, freesans, sans-serif;
  font-size: 16px;
  line-height: 1.6;
  word-wrap: break-word;
}

.markdown-body a {
  background: transparent;
}

.markdown-body a:active,
.markdown-body a:hover {
  outline: 0;
}

.markdown-body b,
.markdown-body strong {
  font-weight: bold;
}

.markdown-body mark {
  background: #ff0;
  color: #000;
  font-style: italic;
  font-weight: bold;
}

.markdown-body sub,
.markdown-body sup {
  font-size: 75%;
  line-height: 0;
  position: relative;
  vertical-align: baseline;
}
.markdown-body sup {
  top: -0.5em;
}
.markdown-body sub {
  bottom: -0.25em;
}

.markdown-body h1 {
  font-size: 2em;
  margin: 0.67em 0;
}

.markdown-body img {
  border: 0;
}

.markdown-body hr {
  -moz-box-sizing: content-box;
  box-sizing: content-box;
  height: 0;
}

.markdown-body pre {
  overflow: auto;
}

.markdown-body code,
.markdown-body kbd,
.markdown-body pre,
.markdown-body samp {
  font-family: monospace, monospace;
  font-size: 1em;
}

.markdown-body input {
  color: inherit;
  font: inherit;
  margin: 0;
}

.markdown-body html input[disabled] {
  cursor: default;
}

.markdown-body input {
  line-height: normal;
}

.markdown-body input[type="checkbox"] {
  box-sizing: border-box;
  padding: 0;
}

.markdown-body table {
  border-collapse: collapse;
  border-spacing: 0;
}

.markdown-body td,
.markdown-body th {
  padding: 0;
}

.markdown-body .codehilitetable {
  border: 0;
  border-spacing: 0;
}

.markdown-body .codehilitetable tr {
  border: 0;
}

.markdown-body .codehilitetable pre,
.markdown-body .codehilitetable div.codehilite {
  margin: 0;
}

.markdown-body .linenos,
.markdown-body .code,
.markdown-body .codehilitetable td {
  border: 0;
  padding: 0;
}

.markdown-body td:not(.linenos) .linenodiv {
  padding: 0 !important;
}

.markdown-body .code {
  width: 100%;
}

.markdown-body .linenos div pre,
.markdown-body .linenodiv pre,
.markdown-body .linenodiv {
  border: 0;
  -webkit-border-radius: 0;
  -moz-border-radius: 0;
  border-radius: 0;
  -webkit-border-top-left-radius: 3px;
  -webkit-border-bottom-left-radius: 3px;
  -moz-border-radius-topleft: 3px;
  -moz-border-radius-bottomleft: 3px;
  border-top-left-radius: 3px;
  border-bottom-left-radius: 3px;
}

.markdown-body .code div pre,
.markdown-body .code div {
  border: 0;
  -webkit-border-radius: 0;
  -moz-border-radius: 0;
  border-radius: 0;
  -webkit-border-top-right-radius: 3px;
  -webkit-border-bottom-right-radius: 3px;
  -moz-border-radius-topright: 3px;
  -moz-border-radius-bottomright: 3px;
  border-top-right-radius: 3px;
  border-bottom-right-radius: 3px;
}

.markdown-body * {
  -moz-box-sizing: border-box;
  box-sizing: border-box;
}

.markdown-body input {
  font: 13px Helvetica, arial, freesans, clean, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol";
  line-height: 1.4;
}

.markdown-body a {
  color: #4183c4;
  text-decoration: none;
}

.markdown-body a:hover,
.markdown-body a:focus,
.markdown-body a:active {
  text-decoration: underline;
}

.markdown-body hr {
  height: 0;
  margin: 15px 0;
  overflow: hidden;
  background: transparent;
  border: 0;
  border-bottom: 1px solid #ddd;
}

.markdown-body hr:before,
.markdown-body hr:after {
  display: table;
  content: " ";
}

.markdown-body hr:after {
  clear: both;
}

.markdown-body h1,
.markdown-body h2,
.markdown-body h3,
.markdown-body h4,
.markdown-body h5,
.markdown-body h6 {
  margin-top: 15px;
  margin-bottom: 15px;
  line-height: 1.1;
}

.markdown-body h1 {
  font-size: 30px;
}

.markdown-body h2 {
  font-size: 21px;
}

.markdown-body h3 {
  font-size: 16px;
}

.markdown-body h4 {
  font-size: 14px;
}

.markdown-body h5 {
  font-size: 12px;
}

.markdown-body h6 {
  font-size: 11px;
}

.markdown-body blockquote {
  margin: 0;
}

.markdown-body ul,
.markdown-body ol {
  padding: 0;
  margin-top: 0;
  margin-bottom: 0;
}

.markdown-body ol ol,
.markdown-body ul ol {
  list-style-type: lower-roman;
}

.markdown-body ul ul ol,
.markdown-body ul ol ol,
.markdown-body ol ul ol,
.markdown-body ol ol ol {
  list-style-type: lower-alpha;
}

.markdown-body dd {
  margin-left: 0;
}

.markdown-body code,
.markdown-body pre,
.markdown-body samp {
  font-family: Consolas, "Liberation Mono", Menlo, Courier, monospace;
  font-size: 12px;
}

.markdown-body pre {
  margin-top: 0;
  margin-bottom: 0;
}

.markdown-body kbd {
  background-color: #e7e7e7;
  background-image: -moz-linear-gradient(#fefefe, #e7e7e7);
  background-image: -webkit-linear-gradient(#fefefe, #e7e7e7);
  background-image: linear-gradient(#fefefe, #e7e7e7);
  background-repeat: repeat-x;
  border-radius: 2px;
  border: 1px solid #cfcfcf;
  color: #000;
  padding: 3px 5px;
  line-height: 10px;
  font: 11px Consolas, "Liberation Mono", Menlo, Courier, monospace;
  display: inline-block;
}

.markdown-body>*:first-child {
  margin-top: 0 !important;
}

.markdown-body>*:last-child {
  margin-bottom: 0 !important;
}

.markdown-body .headeranchor-link {
  position: absolute;
  top: 0;
  bottom: 0;
  left: 0;
  display: block;
  padding-right: 6px;
  padding-left: 30px;
  margin-left: -30px;
}

.markdown-body .headeranchor-link:focus {
  outline: none;
}

.markdown-body h1,
.markdown-body h2,
.markdown-body h3,
.markdown-body h4,
.markdown-body h5,
.markdown-body h6 {
  position: relative;
  margin-top: 1em;
  margin-bottom: 16px;
  font-weight: bold;
  line-height: 1.4;
}

.markdown-body h1 .headeranchor,
.markdown-body h2 .headeranchor,
.markdown-body h3 .headeranchor,
.markdown-body h4 .headeranchor,
.markdown-body h5 .headeranchor,
.markdown-body h6 .headeranchor {
  display: none;
  color: #000;
  vertical-align: middle;
}

.markdown-body h1:hover .headeranchor-link,
.markdown-body h2:hover .headeranchor-link,
.markdown-body h3:hover .headeranchor-link,
.markdown-body h4:hover .headeranchor-link,
.markdown-body h5:hover .headeranchor-link,
.markdown-body h6:hover .headeranchor-link {
  height: 1em;
  padding-left: 8px;
  margin-left: -30px;
  line-height: 1;
  text-decoration: none;
}

.markdown-body h1:hover .headeranchor-link .headeranchor,
.markdown-body h2:hover .headeranchor-link .headeranchor,
.markdown-body h3:hover .headeranchor-link .headeranchor,
.markdown-body h4:hover .headeranchor-link .headeranchor,
.markdown-body h5:hover .headeranchor-link .headeranchor,
.markdown-body h6:hover .headeranchor-link .headeranchor {
  display: inline-block;
}

.markdown-body h1 {
  padding-bottom: 0.3em;
  font-size: 2.25em;
  line-height: 1.2;
  border-bottom: 1px solid #eee;
}

.markdown-body h2 {
  padding-bottom: 0.3em;
  font-size: 1.75em;
  line-height: 1.225;
  border-bottom: 1px solid #eee;
}

.markdown-body h3 {
  font-size: 1.5em;
  line-height: 1.43;
}

.markdown-body h4 {
  font-size: 1.25em;
}

.markdown-body h5 {
  font-size: 1em;
}

.markdown-body h6 {
  font-size: 1em;
  color: #777;
}

.markdown-body p,
.markdown-body blockquote,
.markdown-body ul,
.markdown-body ol,
.markdown-body dl,
.markdown-body table,
.markdown-body pre,
.markdown-body .admonition {
  margin-top: 0;
  margin-bottom: 16px;
}

.markdown-body hr {
  height: 4px;
  padding: 0;
  margin: 16px 0;
  background-color: #e7e7e7;
  border: 0 none;
}

.markdown-body ul,
.markdown-body ol {
  padding-left: 2em;
}

.markdown-body ul ul,
.markdown-body ul ol,
.markdown-body ol ol,
.markdown-body ol ul {
  margin-top: 0;
  margin-bottom: 0;
}

.markdown-body li>p {
  margin-top: 16px;
}

.markdown-body dl {
  padding: 0;
}

.markdown-body dl dt {
  padding: 0;
  margin-top: 16px;
  font-size: 1em;
  font-style: italic;
  font-weight: bold;
}

.markdown-body dl dd {
  padding: 0 16px;
  margin-bottom: 16px;
}

.markdown-body blockquote {
  padding: 0 15px;
  color: #777;
  border-left: 4px solid #ddd;
}

.markdown-body blockquote>:first-child {
  margin-top: 0;
}

.markdown-body blockquote>:last-child {
  margin-bottom: 0;
}

.markdown-body table {
  display: block;
  width: 100%;
  overflow: auto;
  word-break: normal;
  word-break: keep-all;
}

.markdown-body table th {
  font-weight: bold;
}

.markdown-body table th,
.markdown-body table td {
  padding: 6px 13px;
  border: 1px solid #ddd;
}

.markdown-body table tr {
  background-color: #fff;
  border-top: 1px solid #ccc;
}

.markdown-body table tr:nth-child(2n) {
  background-color: #f8f8f8;
}

.markdown-body img {
  max-width: 100%;
  -moz-box-sizing: border-box;
  box-sizing: border-box;
}

.markdown-body code,
.markdown-body samp {
  padding: 0;
  padding-top: 0.2em;
  padding-bottom: 0.2em;
  margin: 0;
  font-size: 85%;
  background-color: rgba(0,0,0,0.04);
  border-radius: 3px;
}

.markdown-body code:before,
.markdown-body code:after {
  letter-spacing: -0.2em;
  content: "\00a0";
}

.markdown-body pre>code {
  padding: 0;
  margin: 0;
  font-size: 100%;
  word-break: normal;
  white-space: pre;
  background: transparent;
  border: 0;
}

.markdown-body .codehilite {
  margin-bottom: 16px;
}

.markdown-body .codehilite pre,
.markdown-body pre {
  padding: 16px;
  overflow: auto;
  font-size: 85%;
  line-height: 1.45;
  background-color: #f7f7f7;
  border-radius: 3px;
}

.markdown-body .codehilite pre {
  margin-bottom: 0;
  word-break: normal;
}

.markdown-body pre {
  word-wrap: normal;
}

.markdown-body pre code {
  display: inline;
  max-width: initial;
  padding: 0;
  margin: 0;
  overflow: initial;
  line-height: inherit;
  word-wrap: normal;
  background-color: transparent;
  border: 0;
}

.markdown-body pre code:before,
.markdown-body pre code:after {
  content: normal;
}

/* Admonition */
.markdown-body .admonition {
  -webkit-border-radius: 3px;
  -moz-border-radius: 3px;
  position: relative;
  border-radius: 3px;
  border: 1px solid #e0e0e0;
  border-left: 6px solid #333;
  padding: 10px 10px 10px 30px;
}

.markdown-body .admonition table {
  color: #333;
}

.markdown-body .admonition p {
  padding: 0;
}

.markdown-body .admonition-title {
  font-weight: bold;
  margin: 0;
}

.markdown-body .admonition>.admonition-title {
  color: #333;
}

.markdown-body .attention>.admonition-title {
  color: #a6d796;
}

.markdown-body .caution>.admonition-title {
  color: #d7a796;
}

.markdown-body .hint>.admonition-title {
  color: #96c6d7;
}

.markdown-body .danger>.admonition-title {
  color: #c25f77;
}

.markdown-body .question>.admonition-title {
  color: #96a6d7;
}

.markdown-body .note>.admonition-title {
  color: #d7c896;
}

.markdown-body .admonition:before,
.markdown-body .attention:before,
.markdown-body .caution:before,
.markdown-body .hint:before,
.markdown-body .danger:before,
.markdown-body .question:before,
.markdown-body .note:before {
  font: normal normal 16px fontawesome-mini;
  -moz-osx-font-smoothing: grayscale;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
  line-height: 1.5;
  color: #333;
  position: absolute;
  left: 0;
  top: 0;
  padding-top: 10px;
  padding-left: 10px;
}

.markdown-body .admonition:before {
  content: "\f056\00a0";
  color: 333;
}

.markdown-body .attention:before {
  content: "\f058\00a0";
  color: #a6d796;
}

.markdown-body .caution:before {
  content: "\f06a\00a0";
  color: #d7a796;
}

.markdown-body .hint:before {
  content: "\f05a\00a0";
  color: #96c6d7;
}

.markdown-body .danger:before {
  content: "\f057\00a0";
  color: #c25f77;
}

.markdown-body .question:before {
  content: "\f059\00a0";
  color: #96a6d7;
}

.markdown-body .note:before {
  content: "\f040\00a0";
  color: #d7c896;
}

.markdown-body .admonition::after {
  content: normal;
}

.markdown-body .attention {
  border-left: 6px solid #a6d796;
}

.markdown-body .caution {
  border-left: 6px solid #d7a796;
}

.markdown-body .hint {
  border-left: 6px solid #96c6d7;
}

.markdown-body .danger {
  border-left: 6px solid #c25f77;
}

.markdown-body .question {
  border-left: 6px solid #96a6d7;
}

.markdown-body .note {
  border-left: 6px solid #d7c896;
}

.markdown-body .admonition>*:first-child {
  margin-top: 0 !important;
}

.markdown-body .admonition>*:last-child {
  margin-bottom: 0 !important;
}

/* progress bar*/
.markdown-body .progress {
  display: block;
  width: 300px;
  margin: 10px 0;
  height: 24px;
  -webkit-border-radius: 3px;
  -moz-border-radius: 3px;
  border-radius: 3px;
  background-color: #ededed;
  position: relative;
  box-shadow: inset -1px 1px 3px rgba(0, 0, 0, .1);
}

.markdown-body .progress-label {
  position: absolute;
  text-align: center;
  font-weight: bold;
  width: 100%; margin: 0;
  line-height: 24px;
  color: #333;
  text-shadow: 1px 1px 0 #fefefe, -1px -1px 0 #fefefe, -1px 1px 0 #fefefe, 1px -1px 0 #fefefe, 0 1px 0 #fefefe, 0 -1px 0 #fefefe, 1px 0 0 #fefefe, -1px 0 0 #fefefe, 1px 1px 2px #000;
  -webkit-font-smoothing: antialiased !important;
  white-space: nowrap;
  overflow: hidden;
}

.markdown-body .progress-bar {
  height: 24px;
  float: left;
  -webkit-border-radius: 3px;
  -moz-border-radius: 3px;
  border-radius: 3px;
  background-color: #96c6d7;
  box-shadow: inset 0 1px 0 rgba(255, 255, 255, .5), inset 0 -1px 0 rgba(0, 0, 0, .1);
  background-size: 30px 30px;
  background-image: -webkit-linear-gradient(
    135deg, rgba(255, 255, 255, .4) 27%,
    transparent 27%,
    transparent 52%, rgba(255, 255, 255, .4) 52%,
    rgba(255, 255, 255, .4) 77%,
    transparent 77%, transparent
  );
  background-image: -moz-linear-gradient(
    135deg,
    rgba(255, 255, 255, .4) 27%, transparent 27%,
    transparent 52%, rgba(255, 255, 255, .4) 52%,
    rgba(255, 255, 255, .4) 77%, transparent 77%,
    transparent
  );
  background-image: -ms-linear-gradient(
    135deg,
    rgba(255, 255, 255, .4) 27%, transparent 27%,
    transparent 52%, rgba(255, 255, 255, .4) 52%,
    rgba(255, 255, 255, .4) 77%, transparent 77%,
    transparent
  );
  background-image: -o-linear-gradient(
    135deg,
    rgba(255, 255, 255, .4) 27%, transparent 27%,
    transparent 52%, rgba(255, 255, 255, .4) 52%,
    rgba(255, 255, 255, .4) 77%, transparent 77%,
    transparent
  );
  background-image: linear-gradient(
    135deg,
    rgba(255, 255, 255, .4) 27%, transparent 27%,
    transparent 52%, rgba(255, 255, 255, .4) 52%,
    rgba(255, 255, 255, .4) 77%, transparent 77%,
    transparent
  );
}

.markdown-body .progress-100plus .progress-bar {
  background-color: #a6d796;
}

.markdown-body .progress-80plus .progress-bar {
  background-color: #c6d796;
}

.markdown-body .progress-60plus .progress-bar {
  background-color: #d7c896;
}

.markdown-body .progress-40plus .progress-bar {
  background-color: #d7a796;
}

.markdown-body .progress-20plus .progress-bar {
  background-color: #d796a6;
}

.markdown-body .progress-0plus .progress-bar {
  background-color: #c25f77;
}

.markdown-body .candystripe-animate .progress-bar{
  -webkit-animation: animate-stripes 3s linear infinite;
  -moz-animation: animate-stripes 3s linear infinite;
  animation: animate-stripes 3s linear infinite;
}

@-webkit-keyframes animate-stripes {
  0% {
    background-position: 0 0;
  }

  100% {
    background-position: 60px 0;
  }
}

@-moz-keyframes animate-stripes {
  0% {
    background-position: 0 0;
  }

  100% {
    background-position: 60px 0;
  }
}

@keyframes animate-stripes {
  0% {
    background-position: 0 0;
  }

  100% {
    background-position: 60px 0;
  }
}

.markdown-body .gloss .progress-bar {
  box-shadow:
    inset 0 4px 12px rgba(255, 255, 255, .7),
    inset 0 -12px 0 rgba(0, 0, 0, .05);
}

/* Multimarkdown Critic Blocks */
.markdown-body .critic_mark {
  background: #ff0;
}

.markdown-body .critic_delete {
  color: #c82829;
  text-decoration: line-through;
}

.markdown-body .critic_insert {
  color: #718c00 ;
  text-decoration: underline;
}

.markdown-body .critic_comment {
  color: #8e908c;
  font-style: italic;
}

.markdown-body .headeranchor {
  font: normal normal 16px octicons-anchor;
  line-height: 1;
  display: inline-block;
  text-decoration: none;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.headeranchor:before {
  content: '\f05c';
}

.markdown-body .task-list-item {
  list-style-type: none;
}

.markdown-body .task-list-item+.task-list-item {
  margin-top: 3px;
}

.markdown-body .task-list-item input {
  margin: 0 4px 0.25em -20px;
  vertical-align: middle;
}

/* Media */
@media only screen and (min-width: 480px) {
  .markdown-body {
    font-size:14px;
  }
}

@media only screen and (min-width: 768px) {
  .markdown-body {
    font-size:16px;
  }
}

@media print {
  .markdown-body * {
    background: transparent !important;
    color: black !important;
    filter:none !important;
    -ms-filter: none !important;
  }

  .markdown-body {
    font-size:12pt;
    max-width:100%;
    outline:none;
    border: 0;
  }

  .markdown-body a,
  .markdown-body a:visited {
    text-decoration: underline;
  }

  .markdown-body .headeranchor-link {
    display: none;
  }

  .markdown-body a[href]:after {
    content: " (" attr(href) ")";
  }

  .markdown-body abbr[title]:after {
    content: " (" attr(title) ")";
  }

  .markdown-body .ir a:after,
  .markdown-body a[href^="javascript:"]:after,
  .markdown-body a[href^="#"]:after {
    content: "";
  }

  .markdown-body pre {
    white-space: pre;
    white-space: pre-wrap;
    word-wrap: break-word;
  }

  .markdown-body pre,
  .markdown-body blockquote {
    border: 1px solid #999;
    padding-right: 1em;
    page-break-inside: avoid;
  }

  .markdown-body .progress,
  .markdown-body .progress-bar {
    -moz-box-shadow: none;
    -webkit-box-shadow: none;
    box-shadow: none;
  }

  .markdown-body .progress {
    border: 1px solid #ddd;
  }

  .markdown-body .progress-bar {
    height: 22px;
    border-right: 1px solid #ddd;
  }

  .markdown-body tr,
  .markdown-body img {
    page-break-inside: avoid;
  }

  .markdown-body img {
    max-width: 100% !important;
  }

  .markdown-body p,
  .markdown-body h2,
  .markdown-body h3 {
    orphans: 3;
    widows: 3;
  }

  .markdown-body h2,
  .markdown-body h3 {
    page-break-after: avoid;
  }
}
</style><title>chapter-8</title></head><body><article class="markdown-body"><h1 id="_1"><a name="user-content-_1" href="#_1" class="headeranchor-link" aria-hidden="true"><span class="headeranchor"></span></a>第八章：聚类</h1>
<p>原文：<a href="http://guidetodatamining.com/chapter-8/">http://guidetodatamining.com/chapter-8/</a></p>
<p>前几章我们学习了如何构建分类系统，使用的是已经标记好类别的数据集进行训练：</p>
<p><img alt="" src="///F://guidetodatamining/guidetodatamining/img/chapter-8/chapter-8-1.png" /></p>
<p>训练完成后我们就可以用来预测了：这个人看起来像是篮球运动员，那个人可能是练体操的；这个人三年内不会患有糖尿病。</p>
<p>可以看到，分类器在训练阶段就已经知道各个类别的名称了。那如果我们不知道呢？如何构建一个能够自动对数据进行分组的系统？比如有1000人，每人有20个特征，我想把这些人分为若干个组。</p>
<p><img alt="" src="///F://guidetodatamining/guidetodatamining/img/chapter-8/chapter-8-2.png" /></p>
<p>这个过程叫做聚类：通过物品特征来计算距离，并自动分类到不同的群集或组中。有两种聚类算法比较常用：</p>
<p><strong>k-means聚类算法</strong></p>
<p>我们会事先告诉这个算法要将数据分成几个组，比如“请把这1000个人分成5个组”，“将这些网页分成15个组”。这种方法就叫k-means，我们会在后面的章节讨论。</p>
<h2 id="_2"><a name="user-content-_2" href="#_2" class="headeranchor-link" aria-hidden="true"><span class="headeranchor"></span></a>层次聚类法</h2>
<p>对于层次聚类法，我们不需要预先指定分类的数量，这个算方法会将每条数据都当作是一个分类，每次迭代的时候合并距离最近的两个分类，直到剩下一个分类为止。因此聚类的结果是：顶层有一个大分类，这个分类下有两个子分类，每个子分类下又有两个子分类，依此类推，层次聚类也因此得命。</p>
<p><img alt="" src="///F://guidetodatamining/guidetodatamining/img/chapter-8/chapter-8-3.png" /></p>
<p>在合并的时候我们会计算两个分类之间的距离，可以采用不同的方法。如下图中的A、B、C三个分类，我们应该将哪两个分类合并起来呢？</p>
<p><img alt="" src="///F://guidetodatamining/guidetodatamining/img/chapter-8/chapter-8-4.png" /></p>
<h3 id="_3"><a name="user-content-_3" href="#_3" class="headeranchor-link" aria-hidden="true"><span class="headeranchor"></span></a>单链聚类</h3>
<p>在单链聚类中，分类之间的距离由两个分类相距最近的两个元素决定。如上图中分类A和分类B的距离由A1和B1的距离决定，因为这个距离小于A1到B2、A2到B1的距离。这样一来我们会将A和B进行合并。</p>
<h3 id="_4"><a name="user-content-_4" href="#_4" class="headeranchor-link" aria-hidden="true"><span class="headeranchor"></span></a>全链聚类</h3>
<p>在全链聚类中，分类之间的距离由两个分类相距最远的两个元素决定。因此上图中分类A和B的距离是A2到B2的距离，最后会将分类B和C进行合并。</p>
<h3 id="_5"><a name="user-content-_5" href="#_5" class="headeranchor-link" aria-hidden="true"><span class="headeranchor"></span></a>平均链接聚类</h3>
<p>在这种聚类方法中，我们通过计算分类之间两两元素的平均距离来判断分类之间的距离，因此上图中会将分类B和C进行合并。</p>
<p><img alt="" src="///F://guidetodatamining/guidetodatamining/img/chapter-8/chapter-8-5.png" /></p>
<blockquote>
<p>下面让我们用单链聚类法举个例子吧！</p>
</blockquote>
<p>我们来用狗的高度和重量来进行聚类：</p>
<p><img alt="" src="///F://guidetodatamining/guidetodatamining/img/chapter-8/chapter-8-6.png" /></p>
<p><img alt="" src="///F://guidetodatamining/guidetodatamining/img/chapter-8/chapter-8-7.png" /></p>
<blockquote>
<p>在计算距离前我们是不是忘了做件事？</p>
</blockquote>
<p><img alt="" src="///F://guidetodatamining/guidetodatamining/img/chapter-8/chapter-8-8.png" /></p>
<blockquote>
<p>标准化！我们先将这些数据转换为修正的标准分。</p>
</blockquote>
<p><img alt="" src="///F://guidetodatamining/guidetodatamining/img/chapter-8/chapter-8-9.png" /></p>
<p>然后我们计算欧几里德距离，图中高亮了一些最短距离：</p>
<p><img alt="" src="///F://guidetodatamining/guidetodatamining/img/chapter-8/chapter-8-10.png" /></p>
<p>根据下面的图表，你能看出哪两个品种的距离最近吗？</p>
<p><img alt="" src="///F://guidetodatamining/guidetodatamining/img/chapter-8/chapter-8-11.png" /></p>
<blockquote>
<p>如果你看出是Border Collie和Portuguese Water Dog最近，那就对了！</p>
</blockquote>
<p><strong>计算过程</strong></p>
<p>第一步：我们找到距离最近的两个元素，对他们进行聚类：</p>
<p><img alt="" src="///F://guidetodatamining/guidetodatamining/img/chapter-8/chapter-8-12.png" /></p>
<p>第二步：再找出距离最近的两个元素，进行聚类：</p>
<p><img alt="" src="///F://guidetodatamining/guidetodatamining/img/chapter-8/chapter-8-13.png" /></p>
<p>第三步：继续重复上面的步骤：</p>
<p><img alt="" src="///F://guidetodatamining/guidetodatamining/img/chapter-8/chapter-8-14.png" /></p>
<p>第四步：继续查找距离最近的元素，发现Border Collie已经属于一个分类的，因此进行如下图所示的合并：</p>
<p><img alt="" src="///F://guidetodatamining/guidetodatamining/img/chapter-8/chapter-8-15.png" /></p>
<blockquote>
<p>这叫树状图，可以用来表示聚类。</p>
</blockquote>
<p><strong>动手实践</strong></p>
<p>你能在下图的基础上继续完成聚类吗？</p>
<p><img alt="" src="///F://guidetodatamining/guidetodatamining/img/chapter-8/chapter-8-16.png" /></p>
<p><img alt="" src="///F://guidetodatamining/guidetodatamining/img/chapter-8/chapter-8-17.png" /></p>
<p><strong>解答</strong></p>
<p><img alt="" src="///F://guidetodatamining/guidetodatamining/img/chapter-8/chapter-8-18.png" /></p>
<p><img alt="" src="///F://guidetodatamining/guidetodatamining/img/chapter-8/chapter-8-19.png" /></p>
<h2 id="_6"><a name="user-content-_6" href="#_6" class="headeranchor-link" aria-hidden="true"><span class="headeranchor"></span></a>编写层次聚类算法</h2>
<p><img alt="" src="///F://guidetodatamining/guidetodatamining/img/chapter-8/chapter-8-20.png" /></p>
<blockquote>
<p>我们可以使用优先队列来实现这个聚类算法。</p>
<p>什么是优先队列呢？</p>
</blockquote>
<p>普通的队列有“先进先出”的规则，比如向队列先后添加Moa、Suzuka、Yui，取出时得到的也是Moa、Suzuka、Yui：</p>
<p><img alt="" src="///F://guidetodatamining/guidetodatamining/img/chapter-8/chapter-8-21.png" /></p>
<p>而对于优先队列，每个元素都可以附加一个优先级，从队列中取出时会得到优先级最高的元素。比如说，我们定义年龄越小优先级越高，以下是插入过程：</p>
<p><img alt="" src="///F://guidetodatamining/guidetodatamining/img/chapter-8/chapter-8-22.png" /></p>
<p>取出的第一个元素是Yui，因为她的年龄最小：</p>
<p><img alt="" src="///F://guidetodatamining/guidetodatamining/img/chapter-8/chapter-8-23.png" /></p>
<p>我们看看Python中如何使用优先队列：</p>
<pre><code class="python">&gt;&gt;&gt; from Queue import PriorityQueue           # 加载优先队列类
&gt;&gt;&gt; singersQueue = PriorityQueue()            # 创建对象
&gt;&gt;&gt; singersQueue.put((16, 'Suzuka Nakamoto')) # 插入元素
&gt;&gt;&gt; singersQueue.put((15, 'Moa Kikuchi'))
&gt;&gt;&gt; singersQueue.put((14, 'Yui Mizuno'))
&gt;&gt;&gt; singersQueue.put((17, 'Ayaka Sasaki'))
&gt;&gt;&gt; singersQueue.get() # 获取第一个元素，即最年轻的歌手Yui。
(14, 'Yui Mizuno')
&gt;&gt;&gt; singersQueue.get()
(15, 'Moa Kikuchi')
&gt;&gt;&gt; singersQueue.get()
(16, 'Suzuka Nakamoto')
&gt;&gt;&gt; singersQueue.get()
(17, 'Ayaka Sasaki')
</code></pre>

<p>在进行聚类时，我们将分类、离它最近的分类、以及距离插入到优先队列中，距离作为优先级。比如上面的犬种示例，Border Collie最近的分类是Portuguese WD，距离是0.232：</p>
<p><img alt="" src="///F://guidetodatamining/guidetodatamining/img/chapter-8/chapter-8-24.png" /></p>
<p>我们将优先队列中距离最小的两个分类取出来，合并成一个分类，并重新插入到优先队列中。比如下图是将Border Collie和Portuguese WD合并后的结果：</p>
<p><img alt="" src="///F://guidetodatamining/guidetodatamining/img/chapter-8/chapter-8-25.png" /></p>
<p>重复这个过程，直到队列中只有一个元素为止。当然，我们插入的数据会复杂一些，请看下面的讲解。</p>
<h3 id="_7"><a name="user-content-_7" href="#_7" class="headeranchor-link" aria-hidden="true"><span class="headeranchor"></span></a>从文件中读取数据</h3>
<p>数据文件是CSV格式的（以逗号分隔），第一行是列名，第一列是犬种，第二列之后是特征值：</p>
<p><img alt="" src="///F://guidetodatamining/guidetodatamining/img/chapter-8/chapter-8-26.png" /></p>
<p>我们用Python的列表结构来存储这些数据，data[0]用来存放所有记录的分类，如data[0][0]是Border Collie，data[0][1]是Boston Terrier。data[1]则是所有记录的高度，data[2]是重量。特征列的数据都会转换成浮点类型，如data[1][0]是20.0，data[2][0]是45.0等。在读取数据时就需要对其进行标准化。此外，我们接下来会使用“下标”这个术语，如第一条记录Border Collie的下标是0，第二条记录Boston Terrier下标是1等。</p>
<h3 id="_8"><a name="user-content-_8" href="#_8" class="headeranchor-link" aria-hidden="true"><span class="headeranchor"></span></a>初始化优先队列</h3>
<p>以Border Collie为例，我们需要计算它和其它犬种的距离，保存在Python字典里：</p>
<pre><code class="python">{1: ((0, 1), 1.0244),  # Border Collie（下标为0）和Boston Terrier（下标为1）之间的距离为1.0244
 2: ((0, 2), 0.463),   # Border Collie和Brittany Spaniel（下标为2）之间的距离为0.463
 ...
 10: ((0, 10), 2.756)} # Border Collie和Yorkshire Terrier的距离为2.756
</code></pre>

<p>此外，我们会记录Border Collie最近的分类及距离：这对犬种是(0, 8)，即下标为0的Border Collie和下标为8的Portuguese WD，距离是0.232。</p>
<h4 id="_9"><a name="user-content-_9" href="#_9" class="headeranchor-link" aria-hidden="true"><span class="headeranchor"></span></a>距离相等的问题以及为何要使用元组</h4>
<p>你也许注意到了，Portuguese WD和Standard Poodle的距离是0.566，Boston Terrier和Brittany Spaniel的距离也是0.566，如果我们通过最短距离来取，很可能会取出Standard Poodle和Boston Terrier进行组合，这显然是错误的，所以我们才会使用元组来存放这对犬种的下标，以作判断。比如说，Portuguese WD的记录是：</p>
<pre><code class="python">['Portuguese Water Dog', 0.566, (8, 9)]
</code></pre>

<p>它的近邻Standard Poodle的记录是：</p>
<pre><code class="python">['Standard Poodle', 0.566, (8, 9)]
</code></pre>

<p>我们可以通过这个元组来判断这两条记录是否是一对。</p>
<h4 id="_10"><a name="user-content-_10" href="#_10" class="headeranchor-link" aria-hidden="true"><span class="headeranchor"></span></a>距离相等的另一个问题</h4>
<p>在介绍优先队列时，我用了歌手的年龄举例，如果他们的年龄相等，取出的顺序又是怎样的呢？</p>
<p><img alt="" src="///F://guidetodatamining/guidetodatamining/img/chapter-8/chapter-8-27.png" /></p>
<p>可以看到，如果年龄相等，优先队列会根据记录中的第二个元素进行判断，即歌手的姓名，并按字母顺序返回，如Avaka会比Moa优先返回。</p>
<p>在犬种示例中，我们让距离成为第一优先级，下标成为第二优先级。因此，我们插入到优先队列的一条完整记录是这样的：</p>
<p><img alt="" src="///F://guidetodatamining/guidetodatamining/img/chapter-8/chapter-8-28.png" /></p>
<h3 id="_11"><a name="user-content-_11" href="#_11" class="headeranchor-link" aria-hidden="true"><span class="headeranchor"></span></a>重复下述步骤，直到仅剩一个分类</h3>
<p>我们从优先队列中取出两个元素，对它们进行合并。如合并Border Collie和Portuguese WD后，会形成一个新的分类：</p>
<pre><code class="python">['Border Collie', 'Portuguese WD']
</code></pre>

<p>然后我们需要计算新的分类和其它分类之间的距离，方法是对取出的两个分类的距离字典进行合并。如第一个分类的距离字段是distanceDict1，第二个分类的是distanceDict2，新的距离字段是newDistanceDict：</p>
<pre><code>初始化newDistanceDict
对于distanceDict1的每一个键值对:
    如果这个键在distanceDict2中存在:
        如果这个键在distanceDict1中的距离要比在distanceDict2中的距离小:
            将distanceDict1中的距离存入newDistanceDict
        否则:
            将distanceDict2中的距离存入newDistanceDict
</code></pre>

<p><img alt="" src="///F://guidetodatamining/guidetodatamining/img/chapter-8/chapter-8-29.png" /></p>
<p>经过计算后，插入到优先队列中的新分类的完整记录是：</p>
<p><img alt="" src="///F://guidetodatamining/guidetodatamining/img/chapter-8/chapter-8-30.png" /></p>
<p><strong>代码实践</strong></p>
<p>你能将上面的算法用Python实现吗？你可以从<a href="///F://guidetodatamining/guidetodatamining/code/chapter-8/hierarchicalClustererTemplate.py">hierarchicalClustererTemplate.py</a>这个文件开始，完成以下步骤：</p>
<ol>
<li>编写init方法，对于每条记录：<ol>
<li>计算该分类和其它分类之间的欧几里得距离；</li>
<li>找出该分类的近邻；</li>
<li>将这些信息放到优先队列的中。</li>
</ol>
</li>
<li>编写cluster方法，重复以下步骤，直至剩下一个分类：<ol>
<li>从优先队列中获取两个元素；</li>
<li>合并；</li>
<li>将合并后的分类放回优先队列中。</li>
</ol>
</li>
</ol>
<p><img alt="" src="///F://guidetodatamining/guidetodatamining/img/chapter-8/chapter-8-31.png" /></p>
<p><strong>解答</strong></p>
<blockquote>
<p>注意，我的实现并不一定是最好的，你可以写出更好的！</p>
</blockquote>
<pre><code class="python">from queue import PriorityQueue
import math

&quot;&quot;&quot;
层次聚类示例代码
&quot;&quot;&quot;

def getMedian(alist):
    &quot;&quot;&quot;计算中位数&quot;&quot;&quot;
    tmp = list(alist)
    tmp.sort()
    alen = len(tmp)
    if (alen % 2) == 1:
        return tmp[alen // 2]
    else:
        return (tmp[alen // 2] + tmp[(alen // 2) - 1]) / 2

def normalizeColumn(column):
    &quot;&quot;&quot;计算修正的标准分&quot;&quot;&quot;
    median = getMedian(column)
    asd = sum([abs(x - median) for x in column]) / len(column)
    result = [(x - median) / asd for x in column]
    return result

class hClusterer:
    &quot;&quot;&quot;该聚类器默认数据的第一列是标签，其它列是数值型的特征。&quot;&quot;&quot;

    def __init__(self, filename):
        file = open(filename)
        self.data = {}
        self.counter = 0
        self.queue = PriorityQueue()
        lines = file.readlines()
        file.close()
        header = lines[0].split(',')
        self.cols = len(header)
        self.data = [[] for i in range(len(header))]
        for line in lines[1:]:
            cells = line.split(',')
            toggle = 0
            for cell in range(self.cols):
                if toggle == 0:
                   self.data[cell].append(cells[cell])
                   toggle = 1
                else:
                    self.data[cell].append(float(cells[cell]))
        # 标准化特征列（即跳过第一列）
        for i in range(1, self.cols):
                self.data[i] = normalizeColumn(self.data[i])

        ###
        ###  数据已经读入内存并做了标准化，对于每一条记录，将执行以下步骤：
        ###     1. 计算该分类和其他分类的距离，如当前分类的下标是1，
        ###        它和下标为2及下标为3的分类之间的距离用以下形式表示：
        ###        {2: ((1, 2), 1.23),  3: ((1, 3), 2.3)... }
        ###     2. 找出距离最近的分类；
        ###     3. 将该分类插入到优先队列中。
        ###

        # 插入队列
        rows = len(self.data[0])              

        for i in range(rows):
            minDistance = 99999
            nearestNeighbor = 0
            neighbors = {}
            for j in range(rows):
                if i != j:
                    dist = self.distance(i, j)
                    if i &lt; j:
                        pair = (i,j)
                    else:
                        pair = (j,i)
                    neighbors[j] = (pair, dist)
                    if dist &lt; minDistance:
                        minDistance = dist
                        nearestNeighbor = j
                        nearestNum = j
            # 记录这两个分类的配对信息
            if i &lt; nearestNeighbor:
                nearestPair = (i, nearestNeighbor)
            else:
                nearestPair = (nearestNeighbor, i)

            # 插入优先队列
            self.queue.put((minDistance, self.counter,
                            [[self.data[0][i]], nearestPair, neighbors]))
            self.counter += 1

    def distance(self, i, j):
        sumSquares = 0
        for k in range(1, self.cols):
            sumSquares += (self.data[k][i] - self.data[k][j])**2
        return math.sqrt(sumSquares)

    def cluster(self):
         done = False
         while not done:
             topOne = self.queue.get()
             nearestPair = topOne[2][1]
             if not self.queue.empty():
                 nextOne = self.queue.get()
                 nearPair = nextOne[2][1]
                 tmp = []

                 ##  我从队列中取出了两个元素：topOne和nextOne，
                 ##  检查这两个分类是否是一对，如果不是就继续从优先队列中取出元素，
                 ##  直至找到topOne的配对分类为止。
                 while nearPair != nearestPair:
                     tmp.append((nextOne[0], self.counter, nextOne[2]))
                     self.counter += 1
                     nextOne = self.queue.get()
                     nearPair = nextOne[2][1]

                 ## 将不处理的元素退回给优先队列
                 for item in tmp:
                     self.queue.put(item)

                 if len(topOne[2][0]) == 1:
                     item1 = topOne[2][0][0]
                 else:
                     item1 = topOne[2][0]
                 if len(nextOne[2][0]) == 1:
                     item2 = nextOne[2][0][0]
                 else:
                     item2 = nextOne[2][0]
                 ##  curCluster即合并后的分类
                 curCluster = (item1, item2)

                 ## 对于这个新的分类需要做两件事情：首先找到离它最近的分类，然后合并距离字典。
                 ## 如果item1和元素23的距离是2，item2和元素23的距离是4，我们取较小的那个距离，即单链聚类。
                 minDistance = 99999
                 nearestPair = ()
                 nearestNeighbor = ''
                 merged = {}
                 nNeighbors = nextOne[2][2]
                 for (key, value) in topOne[2][2].items():
                    if key in nNeighbors:
                        if nNeighbors[key][1] &lt; value[1]:
                             dist =  nNeighbors[key]
                        else:
                            dist = value
                        if dist[1] &lt; minDistance:
                             minDistance =  dist[1]
                             nearestPair = dist[0]
                             nearestNeighbor = key
                        merged[key] = dist

                 if merged == {}:
                    return curCluster
                 else:
                    self.queue.put( (minDistance, self.counter,
                                     [curCluster, nearestPair, merged]))
                    self.counter += 1

def printDendrogram(T, sep=3):
    &quot;&quot;&quot;打印二叉树状图。树的每个节点是一个二元组。这个方法摘自：
    http://code.activestate.com/recipes/139422-dendrogram-drawing/&quot;&quot;&quot;

    def isPair(T):
        return type(T) == tuple and len(T) == 2

    def maxHeight(T):
        if isPair(T):
            h = max(maxHeight(T[0]), maxHeight(T[1]))
        else:
            h = len(str(T))
        return h + sep

    activeLevels = {}

    def traverse(T, h, isFirst):
        if isPair(T):
            traverse(T[0], h-sep, 1)
            s = [' ']*(h-sep)
            s.append('|')
        else:
            s = list(str(T))
            s.append(' ')

        while len(s) &lt; h:
            s.append('-')

        if (isFirst &gt;= 0):
            s.append('+')
            if isFirst:
                activeLevels[h] = 1
            else:
                del activeLevels[h]

        A = list(activeLevels)
        A.sort()
        for L in A:
            if len(s) &lt; L:
                while len(s) &lt; L:
                    s.append(' ')
                s.append('|')

        print (''.join(s))    

        if isPair(T):
            traverse(T[1], h-sep, 0)

    traverse(T, maxHeight(T), -1)


filename = '/Users/raz/Dropbox/guide/data/dogs.csv'

hg = hClusterer(filename)
cluster = hg.cluster()
printDendrogram(cluster)
</code></pre>

<p>运行结果和我们手算的一致：</p>
<p><img alt="" src="///F://guidetodatamining/guidetodatamining/img/chapter-8/chapter-8-32.png" /></p>
<p><strong>动手实践</strong></p>
<p><a href="///F://guidetodatamining/guidetodatamining/code/chapter-8/cereal.csv">这里</a>提供了77种早餐麦片的营养信息，包括以下几项：</p>
<ul>
<li>麦片名称</li>
<li>热量</li>
<li>蛋白质</li>
<li>脂肪</li>
<li>纳</li>
<li>纤维</li>
<li>碳水化合物</li>
<li>糖</li>
<li>钾</li>
<li>维生素</li>
</ul>
<p><img alt="" src="///F://guidetodatamining/guidetodatamining/img/chapter-8/chapter-8-33.png" /></p>
<p>请对这个数据集进行层次聚类：</p>
<ul>
<li>哪种麦片和Trix最相近？</li>
<li>与Muesli Raisins &amp; Almonds最相近的是？</li>
</ul>
<blockquote>
<p>数据集来自：<a href="http://lib.stat.cmu.edu/DASL/Datafiles/Cereals.html">http://lib.stat.cmu.edu/DASL/Datafiles/Cereals.html</a></p>
</blockquote>
<p><strong>结果</strong></p>
<p>我们只需将代码中的文件名替换掉就可以了，结果如下：</p>
<p><img alt="" src="///F://guidetodatamining/guidetodatamining/img/chapter-8/chapter-8-34.png" /></p>
<p>因此Trix和Fruity Pebbles最相似（你可以去买这两种麦片尝尝）。Muesli Raisins &amp; Almonds和Muesli Peaches &amp; Pecans最相似。</p>
<p><img alt="" src="///F://guidetodatamining/guidetodatamining/img/chapter-8/chapter-8-35.png" /></p>
<blockquote>
<p>好了，这就是层次聚类算法，很简单吧！</p>
</blockquote>
<h2 id="k-means"><a name="user-content-k-means" href="#k-means" class="headeranchor-link" aria-hidden="true"><span class="headeranchor"></span></a>k-means聚类算法</h2>
<p>使用k-means算法时需要指定分类的数量，这也是算法名称中“k”的由来。</p>
<p><img alt="" src="///F://guidetodatamining/guidetodatamining/img/chapter-8/chapter-8-36.png" /></p>
<blockquote>
<p>k-means是Lloyd博士在1957年提出的，虽然这个算法已有50年的历史，但却是当前最流行的聚类算法！</p>
</blockquote>
<p>下面让我们来了解一下k-means聚类过程：</p>
<p><img alt="" src="///F://guidetodatamining/guidetodatamining/img/chapter-8/chapter-8-37.png" /></p>
<ol>
<li>我们想将图中的记录分成三个分类（即k=3），比如上文提到的犬种数据，坐标轴分别是身高和体重。</li>
<li>由于k=3，我们随机选取三个点来作为聚类的起始点（分类的中心点），并用红黄蓝三种颜色标识。</li>
<li>然后，我们根据其它点到中心点的距离来进行分配，这样就能将这些点分成三类了。</li>
<li>计算这些分类的中心点，以此作为下一次计算的起始点。重复这个过程，直到中心点不再变动，或迭代次数超过某个阈值为止。</li>
</ol>
<p>所以k-means算法可概括为：</p>
<ol>
<li>随机选取k个元素作为中心点；</li>
<li>根据距离将各个点分配给中心点；</li>
<li>计算新的中心点；</li>
<li>重复2、3，直至满足条件。</li>
</ol>
<p>我们来看一个示例，将以下点分成两个分类：</p>
<p><img alt="" src="///F://guidetodatamining/guidetodatamining/img/chapter-8/chapter-8-38.png" /></p>
<p><strong>第一步 随机选取中心点</strong></p>
<p>我们选取(1, 4)作为分类1的中心点，(4, 2)作为分类2的中心点；</p>
<p><strong>第二步 将各点分配给中心点</strong></p>
<p>可以用各类距离计算公式，为简单起见，这里我们使用曼哈顿距离：</p>
<p><img alt="" src="///F://guidetodatamining/guidetodatamining/img/chapter-8/chapter-8-39.png" /></p>
<p>聚类结果如下：</p>
<p><img alt="" src="///F://guidetodatamining/guidetodatamining/img/chapter-8/chapter-8-40.png" /></p>
<p><strong>第三步 更新中心点</strong></p>
<p>通过计算平均值来更新中心点，如x轴的均值是：</p>
<p>(1 + 1 + 2) / 3 = 4 / 3 = 1.33</p>
<p>y轴是：</p>
<p>(2 + 4 + 3) / 3 = 9 / 3 = 3</p>
<p>因此分类1的中心点是(1.33, 3)。计算得到分类2的中心点是(4, 2.4)。</p>
<p><strong>第四步 重复前面两步</strong></p>
<p>两个分类的中心点由(1, 4)、(4, 2)变为了(1.33, 3)、(4, 2.4)，我们使用新的中心点重新计算。</p>
<p><strong>重复第二步 将各点分配给中心点</strong></p>
<p>同样是计算曼哈顿距离：</p>
<p><img alt="" src="///F://guidetodatamining/guidetodatamining/img/chapter-8/chapter-8-41.png" /></p>
<p>新的聚类结果是：</p>
<p><img alt="" src="///F://guidetodatamining/guidetodatamining/img/chapter-8/chapter-8-42.png" /></p>
<p><strong>重复第三步 更新中心点</strong></p>
<ul>
<li>分类1：(1.5, 2.75)</li>
<li>分类2：(4.5, 2.5)</li>
</ul>
<p><strong>重复第二步 将各点分配给中心点</strong></p>
<p><img alt="" src="///F://guidetodatamining/guidetodatamining/img/chapter-8/chapter-8-43.png" /></p>
<p><img alt="" src="///F://guidetodatamining/guidetodatamining/img/chapter-8/chapter-8-44.png" /></p>
<p><strong>重复第三步 更新中心点</strong></p>
<ul>
<li>分类1：(1.5, 2.75)</li>
<li>分类2：(4.5, 2.5)</li>
</ul>
<p>可以看到中心点并没有改变，所以计算也就结束了。</p>
<p><img alt="" src="///F://guidetodatamining/guidetodatamining/img/chapter-8/chapter-8-45.png" /></p>
<blockquote>
<p>当中心点不再变化时，或者说不再有某个点从一个分类转移到另一个分类时，我们就会停止计算。这个时候我们称该算法已经收敛。算法运行过程中，中心点的大幅转移是在前几次迭代中产生的，后面的迭代中变动的幅度就会减小。也就是说，k-means算法的重点是在前期迭代，而后期的迭代只是细微的调整。</p>
</blockquote>
<p><img alt="" src="///F://guidetodatamining/guidetodatamining/img/chapter-8/chapter-8-46.png" /></p>
<blockquote>
<p>基于k-means的这种特点，我们可以将“没有点发生转移”弱化成“少于1%的点发生转移”来作为计算停止条件，这也是最普遍的做法。</p>
</blockquote>
<p><img alt="" src="///F://guidetodatamining/guidetodatamining/img/chapter-8/chapter-8-47.png" /></p>
<blockquote>
<p>k-means好简单呀！</p>
</blockquote>
<p><strong>扩展阅读</strong></p>
<p>k-means是一种最大期望算法，这类算法会在“期望”和“最大化”两个阶段不断迭代。比如k-means的期望阶段是将各个点分配到它们所“期望”的分类中，然后在最大化阶段重新计算中心点的位置。如果你对此感兴趣，可以前去阅读<a href="http://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm">维基百科</a>上的词条。</p>
<h3 id="_12"><a name="user-content-_12" href="#_12" class="headeranchor-link" aria-hidden="true"><span class="headeranchor"></span></a>登山式算法</h3>
<p>再继续讨论k-means算法之前，我想先介绍一下登山式算法。</p>
<p><img alt="" src="///F://guidetodatamining/guidetodatamining/img/chapter-8/chapter-8-48.png" /></p>
<p>假设我们想要登上一座山的顶峰，可以通过以下步骤实现：</p>
<ol>
<li>在山上随机选取一个点作为开始；</li>
<li>向高处爬一点；</li>
<li>重复第2步，直到没有更高的点。</li>
</ol>
<p>这种做法看起来很合理，比如对于下图所示的山峰：</p>
<p><img alt="" src="///F://guidetodatamining/guidetodatamining/img/chapter-8/chapter-8-49.png" /></p>
<p>无论我们从哪个点开始攀登，最终都可以达到顶峰。</p>
<p>但对于下面这张图：</p>
<p><img alt="" src="///F://guidetodatamining/guidetodatamining/img/chapter-8/chapter-8-50.png" /></p>
<p>所以说，这种简单的登山式算法并不一定能得到最优解。</p>
<p>k-means就是这样一种算法，它不能保证最终结果是最优的，因为我们一开始选择的中心点是随机的，很有可能就会选到上面的A点，最终获得局部最优解B点。因此，最终的聚类结果和起始点的选择有很大关系。但尽管如此，k-means通常还是能够获得良好的结果的。</p>
<p><img alt="" src="///F://guidetodatamining/guidetodatamining/img/chapter-8/chapter-8-51.png" /></p>
<blockquote>
<p>那我们如何比较不同的聚类结果呢？</p>
</blockquote>
<h3 id="sse"><a name="user-content-sse" href="#sse" class="headeranchor-link" aria-hidden="true"><span class="headeranchor"></span></a>误差平方和（SSE）</h3>
<p>我们可以使用误差平方和（或称离散程度）来评判聚类结果的好坏，它的计算方法是：计算每个点到中心点的距离平方和。</p>
<p><img alt="" src="///F://guidetodatamining/guidetodatamining/img/chapter-8/chapter-8-52.png" /></p>
<p>上面的公式中，第一个求和符号是遍历所有的分类，比如i=1时计算第一个分类，i=2时计算第二个分类，直到计算第k个分类；第二个求和符号是遍历分类中所有的点；Dist指代距离计算公式（如曼哈顿距离、欧几里得距离）；计算数据点x和中心点c<sub>i</sub>之间的距离，平方后相加。</p>
<p>假设我们对同一数据集使用了两次k-means聚类，每次选取的起始点不一样，想知道最后得到的聚类结果哪个更优，就可以计算和比较SSE，结果小的效果好。</p>
<p><img alt="" src="///F://guidetodatamining/guidetodatamining/img/chapter-8/chapter-8-53.png" /></p>
<blockquote>
<p>下面让我们开始编程吧！</p>
</blockquote>
<pre><code class="python">import math
import random 


&quot;&quot;&quot;
K-means算法
&quot;&quot;&quot;

def getMedian(alist):
    &quot;&quot;&quot;计算中位数&quot;&quot;&quot;
    tmp = list(alist)
    tmp.sort()
    alen = len(tmp)
    if (alen % 2) == 1:
        return tmp[alen // 2]
    else:
        return (tmp[alen // 2] + tmp[(alen // 2) - 1]) / 2


def normalizeColumn(column):
    &quot;&quot;&quot;计算修正的标准分&quot;&quot;&quot;
    median = getMedian(column)
    asd = sum([abs(x - median) for x in column]) / len(column)
    result = [(x - median) / asd for x in column]
    return result


class kClusterer:
    &quot;&quot;&quot;kMeans聚类算法，第一列是分类，其余列是数值型特征&quot;&quot;&quot;

    def __init__(self, filename, k):
        &quot;&quot;&quot;k是分类的数量，该函数完成以下功能：
           1. 读取filename的文件内容
           2. 按列存储到self.data变量中
           3. 计算修正的标准分
           4. 随机选取起始点
           5. 将各个点分配给中心点
        &quot;&quot;&quot;
        file = open(filename)
        self.data = {}
        self.k = k
        self.counter = 0
        self.iterationNumber = 0
        # 用于跟踪本次迭代有多少点的分类发生了变动
        self.pointsChanged = 0
        # 误差平方和
        self.sse = 0
        #
        # 读取文件
        #
        lines = file.readlines()
        file.close()
        header = lines[0].split(',')
        self.cols = len(header)
        self.data = [[] for i in range(len(header))]
        # 按列存储数据，如self.data[0]是第一列的数据，
        # self.data[0][10]是第一列第十行的数据。
        for line in lines[1:]:
            cells = line.split(',')
            toggle = 0
            for cell in range(self.cols):
                if toggle == 0:
                   self.data[cell].append(cells[cell])
                   toggle = 1
                else:
                    self.data[cell].append(float(cells[cell]))

        self.datasize = len(self.data[1])
        self.memberOf = [-1 for x in range(len(self.data[1]))]
        #
        # 标准化
        #
        for i in range(1, self.cols):
                self.data[i] = normalizeColumn(self.data[i])

        # 随机选取起始点
        random.seed()
        self.centroids = [[self.data[i][r]  for i in range(1, len(self.data))]
                           for r in random.sample(range(len(self.data[0])),
                                                 self.k)]
        self.assignPointsToCluster()



    def updateCentroids(self):
        &quot;&quot;&quot;根据分配结果重新确定聚类中心点&quot;&quot;&quot;
        members = [self.memberOf.count(i) for i in range(len(self.centroids))]
        self.centroids = [[sum([self.data[k][i]
                                for i in range(len(self.data[0]))
                                if self.memberOf[i] == centroid])/members[centroid]
                           for k in range(1, len(self.data))]
                          for centroid in range(len(self.centroids))] 



    def assignPointToCluster(self, i):
        &quot;&quot;&quot;根据距离计算所属中心点&quot;&quot;&quot;
        min = 999999
        clusterNum = -1
        for centroid in range(self.k):
            dist = self.euclideanDistance(i, centroid)
            if dist &lt; min:
                min = dist
                clusterNum = centroid
        # 跟踪变动的点
        if clusterNum != self.memberOf[i]:
            self.pointsChanged += 1
        # 计算距离平方和
        self.sse += min**2
        return clusterNum

    def assignPointsToCluster(self):
        &quot;&quot;&quot;分配所有的点&quot;&quot;&quot;
        self.pointsChanged = 0
        self.sse = 0
        self.memberOf = [self.assignPointToCluster(i)
                         for i in range(len(self.data[1]))]



    def euclideanDistance(self, i, j):
        &quot;&quot;&quot;计算欧几里得距离&quot;&quot;&quot;
        sumSquares = 0
        for k in range(1, self.cols):
            sumSquares += (self.data[k][i] - self.centroids[j][k-1])**2
        return math.sqrt(sumSquares)

    def kCluster(self):
        &quot;&quot;&quot;开始进行聚类，重复以下步骤：
        1. 更新中心点
        2. 重新分配
        直至变动的点少于1%。
        &quot;&quot;&quot;
        done = False

        while not done:
            self.iterationNumber += 1
            self.updateCentroids()
            self.assignPointsToCluster()
            #
            # 如果变动的点少于1%则停止迭代
            #
            if float(self.pointsChanged) / len(self.memberOf) &lt;  0.01:
                done = True
        print(&quot;Final SSE: %f&quot; % self.sse)

    def showMembers(self):
        &quot;&quot;&quot;输出结果&quot;&quot;&quot;
        for centroid in range(len(self.centroids)):
             print (&quot;\n\nClass %i\n========&quot; % centroid)
             for name in [self.data[0][i]  for i in range(len(self.data[0]))
                          if self.memberOf[i] == centroid]:
                 print (name)

##
## 对犬种数据进行聚类，令k=3
###
# 请自行修改文件路径
km = kClusterer('../../data/dogs.csv', 3)
km.kCluster()
km.showMembers()
</code></pre>

<p><img alt="" src="///F://guidetodatamining/guidetodatamining/img/chapter-8/chapter-8-54.png" /></p>
<blockquote>
<p>我们来分析一下这段代码。</p>
</blockquote>
<p>犬种数据用表格来展示是这样的，身高和体重都做了标准化：</p>
<p><img alt="" src="///F://guidetodatamining/guidetodatamining/img/chapter-8/chapter-8-55.png" /></p>
<p>因为需要按列存储，转化后的Python格式是这样的：</p>
<pre><code class="python">data = [['Border Collie', 'Bosten Terrier', 'Brittany Spaniel', ...],
        [0, -0.7213, -0.3607, ...],
        [-0.1455, -0.7213, -0.4365, ...],
        ...]
</code></pre>

<p>我们在层次聚类中用的也是此法，这样做的好处是能够方便地应用不同的数学函数。比如计算中位数和计算标准分的函数，都是以列表作为参数的：</p>
<pre><code class="python">&gt;&gt;&gt; normalizeColumn([8, 6, 4, 2])
[1.5, 0.5, -0.5, -1.5]
</code></pre>

<p><code>__init__</code>函数首先将文件读入进来，按列存储，并进行标准化。随后，它会选取k个起始点，并将记录中的点分配给这些中心点。<code>kCluster</code>函数则开始迭代计算中心点的新位置，直到少于1%的点发生变动为止。</p>
<p>程序的运行结果如下：</p>
<pre><code>Final SSE: 5.243159

Class 0
=======
Bullmastiff
Great Dane

Class 1
=======
Boston Terrier
Chihuahua
Yorkshire Terrier

Class 2
=======
Border Collie
Brittany Spaniel
German Shepherd
Golden Retriever
Portuguese Water Dog
Standard Poodle
</code></pre>

<p>聚类结果非常棒！</p>
<p><strong>动手实践</strong></p>
<p>用上面的聚类程序来对麦片数据集进行聚类，令k=4，并回答以下问题：</p>
<ol>
<li>甜味麦片都被聚类到一起了吗，如Cap&rsquo;n&rsquo;Crunch, Cocoa Puffs, Froot Loops, Lucky Charms？</li>
<li>麸类麦片聚到一起了吗，如100% Bran, All-Bran, All-Bran with Extra Fiber, Bran Chex？</li>
<li>Cheerios被分到了哪个类别，是不是一直和Special K一起？</li>
</ol>
<p>再来对加仑公里数的数据进行聚类，令k=8。运行结果大致令人满意，但有时候会出现记录数为空的分类。</p>
<p><img alt="" src="///F://guidetodatamining/guidetodatamining/img/chapter-8/chapter-8-56.png" /></p>
<blockquote>
<p>我要求聚类成8个分类，但其中一个是空的，肯定代码有问题！</p>
</blockquote>
<p>我们用示例来看这个问题，假设需要将以下8个点分成3个类别：</p>
<p><img alt="" src="///F://guidetodatamining/guidetodatamining/img/chapter-8/chapter-8-57.png" /></p>
<p>我们选取1、7、8作为起始点，因此第一次聚类的结果是：</p>
<p><img alt="" src="///F://guidetodatamining/guidetodatamining/img/chapter-8/chapter-8-58.png" /></p>
<p>随后，我们重新计算中心点，即下图中的加号：</p>
<p><img alt="" src="///F://guidetodatamining/guidetodatamining/img/chapter-8/chapter-8-59.png" /></p>
<p>这时，6离蓝色中心点较近，7离绿色中心点较近，因此粉色的分类就为空了。</p>
<p>所以说，虽然我们指定了k的值，但不代表最终结果就会有k个分类。这通常是好事，比如上面的例子中，看起来就应该要分成两类。如果有1000条数据，我们指定k=10，但结果有两个为空，那很有可能这个数据集本来就该分成8个类别，因此可以尝试用k=8来重新计算。</p>
<p>另一方面，如果你要求分类都不为空，那就需要改变一下算法：当发现空的分类时，就重新指定这个分类的中心点。一种做法是选取离这个中心点最远的点，比如上面的例子中，发现粉色分类为空，就将中心点变为点1，因为它离粉色中心点最远。</p>
<p><img alt="" src="///F://guidetodatamining/guidetodatamining/img/chapter-8/chapter-8-60.png" /></p>
<h3 id="k-means_1"><a name="user-content-k-means_1" href="#k-means_1" class="headeranchor-link" aria-hidden="true"><span class="headeranchor"></span></a>k-means++</h3>
<p>前面我们提到k-means是50年代发明的算法，它的实现并不复杂，但仍是现今最流行的聚类算法。不过它也有一个明显的缺点。在算法一开始需要<strong>随机</strong>选取k个起始点，正是这个随机会有问题。有时选取的点能产生最佳结果，而有时会让结果变得很差。k-means++则改进了起始点的选取过程，其余的和k-means一致。</p>
<p>以下是k-means++选取起始点的过程：</p>
<ol>
<li>随机选取一个点；</li>
<li>重复以下步骤，直到选完k个点：<ol>
<li>计算每个数据点（dp）到各个中心点的距离（D），选取最小的值，记为D(dp)；</li>
<li>根据D(dp)的概率来随机选取一个点作为中心点。</li>
</ol>
</li>
</ol>
<p>我们来讲解一下何为“根据D(dp)的概率来随机选取”。假设选取过程进行到一半，已经选出了两个点，现在需要选第三个。假设还有五个点可供选择，它们离已有的两个中心点的距离是：</p>
<p><img alt="" src="///F://guidetodatamining/guidetodatamining/img/chapter-8/chapter-8-61.png" /></p>
<p><em>Dc1表示到中心点1的距离，Dc2表示到中心点2的距离。</em></p>
<p>我们选取最小的距离：</p>
<p><img alt="" src="///F://guidetodatamining/guidetodatamining/img/chapter-8/chapter-8-62.png" /></p>
<p>然后将这些数值转化成总和为1的权重值，做法就是将每个距离除以距离的和（20），得到：</p>
<p><img alt="" src="///F://guidetodatamining/guidetodatamining/img/chapter-8/chapter-8-63.png" /></p>
<p>我们可以通过转盘游戏来理解：</p>
<p><img alt="" src="///F://guidetodatamining/guidetodatamining/img/chapter-8/chapter-8-64.png" /></p>
<p>比如我们扔个球到这个转盘里，它停在哪个颜色就选取这个点作为新的中心点。这就叫做“根据D(dp)的概率来随机选取”。</p>
<p>比如我们有以下Python数据：</p>
<pre><code class="python">data = [('dp1', 0.25), ('dp2', 0.4), ('dp3', 0.1),
        ('dp4', 0.15), ('dp5', 0.1)]
</code></pre>

<p>然后来编写一个函数来完成选取过程：</p>
<pre><code class="python">import random
random.seed()

def roulette(datalist):
    i = 0
    soFar = datalist[0][1]
    ball = random.random()
    while soFar &lt; ball:
        i += 1
        soFar += datalist[i][1]
    return datalist[i][0]
</code></pre>

<p>如果这个函数运行正确，我们选取100次的话，其中25次应该是dp1，40次是dp2，10次是dp3，15次是dp4，10次是dp5。让我们来测试一下：</p>
<pre><code class="python">import collections
results = collections.defaultdict(int)
for i in range(100):
    results[roulette(data)] += 1
print results

{'dp5': 11, 'dp4': 15, 'dp3': 10, 'dp2': 38, 'dp1': 26}
</code></pre>

<p>结果是符合预期的！</p>
<p>k-means++选取起始点的方法总结下来就是：第一个点还是随机的，但后续的点就会尽量选择离现有中心点更远的点。</p>
<p><img alt="" src="///F://guidetodatamining/guidetodatamining/img/chapter-8/chapter-8-65.png" /></p>
<blockquote>
<p>好了，下面让我们开始写代码吧！</p>
</blockquote>
<p><strong>代码实践</strong></p>
<p>你能用Python实现k-means++算法吗？k-means++和k-means的唯一区别就是起始点的选取过程，你需要做的是将下面的代码：</p>
<pre><code class="python">self.centroids = [[self.data[i][r] for i in range(1, len(self.data))]
                   for r in random.sample(range(len(self.data[0])),
                                          self.k)]
</code></pre>

<p>替换为：</p>
<pre><code class="python">self.selectInitialCentroids()
</code></pre>

<p>你的任务就是编写这个函数！</p>
<p><img alt="" src="///F://guidetodatamining/guidetodatamining/img/chapter-8/chapter-8-66.png" /></p>
<p><strong>解答</strong></p>
<pre><code class="python">def distanceToClosestCentroid(self, point, centroidList):
    result = self.eDistance(point, centroidList[0])
    for centroid in centroidList[1:]:
        distance = self.eDistance(point, centroid)
        if distance &lt; result:
            result = distance
    return result

def selectInitialCentroids(self):
    &quot;&quot;&quot;实现k-means++算法中的起始点选取过程&quot;&quot;&quot;
    centroids = []
    total = 0
    # 首先随机选取一个点
    current = random.choice(range(len(self.data[0])))
    centroids.append(current)
    # 开始选取剩余的点
    for i in range(0, self.k - 1):
        # 计算每个点到最近的中心点的距离
        weights = [self.distanceToClosestCentroid(x, centroids) 
                   for x in range(len(self.data[0]))]
        total = sum(weights)
        # 转换为权重
        weights = [x / total for x in weights]
        # 开始随机选取
        num = random.random()
        total = 0
        x = -1
        # 模拟轮盘游戏
        while total &lt; num:
            x += 1
            total += weights[x]
        entroids.append(x)
    self.centroids = [[self.data[i][r]  for i in range(1, len(self.data))]
                      for r in centroids]
</code></pre>

<h2 id="_13"><a name="user-content-_13" href="#_13" class="headeranchor-link" aria-hidden="true"><span class="headeranchor"></span></a>安然事件</h2>
<p>你应该还对这件事有些印象吧？安然公司曾是一家超大型企业，有着千亿元的收入和两万名员工（微软只有220亿收入）。由于管理体制的破败和受贿，包括人为制造能源危机致使加州大停电，安然公司最终面临破产，大批人员被判入狱。有一部名为“The Smartest Guys in the Room”的纪录片，读者可以到Netflix或亚马逊上观看。</p>
<p><img alt="" src="///F://guidetodatamining/guidetodatamining/img/chapter-8/chapter-8-67.png" /></p>
<blockquote>
<p>安然事件的确挺有趣的，不过这和数据挖掘有什么关系呢？</p>
</blockquote>
<p><img alt="" src="///F://guidetodatamining/guidetodatamining/img/chapter-8/chapter-8-68.png" /></p>
<blockquote>
<p>在调查过程中，美国联邦能源管理委员会收获了60万封公司内部邮件。这些邮件可以从网络上下载：</p>
<p><a href="http://en.wikipedia.org/wiki/Enron_Corpus">http://en.wikipedia.org/wiki/Enron_Corpus</a></p>
<p><a href="https://www.cs.cmu.edu/~./enron/">https://www.cs.cmu.edu/~./enron/</a></p>
</blockquote>
<p>我们来用其中的一小部分数据来举例，下表整理了一些公司人员互通邮件的次数：</p>
<p><img alt="" src="///F://guidetodatamining/guidetodatamining/img/chapter-8/chapter-8-69.png" /></p>
<p>可以在<a href="///F://guidetodatamining/guidetodatamining/code/chapter-8/enrondata.txt">这里</a>下载缩减后的数据集。完整的数据在<a href="http://guidetodatamining.com/guide/data/enronMongoDump.zip">这里</a>，超过300MB。</p>
<p><strong>动手实践</strong></p>
<p>你能使用层次聚类算法将这些人分成若干类别吗？</p>
<p><strong>解答</strong></p>
<p>我们会通过两个人收发邮件的对象来计算相似度。比如我经常和Ann、Ben、Clara等人通信，你也一样，那么我俩就是相似的：</p>
<p><img alt="" src="///F://guidetodatamining/guidetodatamining/img/chapter-8/chapter-8-70.png" /></p>
<p>但如果将你我之间的通信也计算进去：</p>
<p><img alt="" src="///F://guidetodatamining/guidetodatamining/img/chapter-8/chapter-8-71.png" /></p>
<p>可以看到，你向我发送了190次邮件，而我只向自己发送了2封邮件。用欧几里得距离来计算的话，在不包含me和you这两列时，我们的距离是46，包含后距离是269！因此在计算两人的距离时需要排除这个因素：</p>
<pre><code class="python">def distance(self, i, j):
    # 针对安然数据进行的修正
    sumSquares = 0
    for i in range(1, self.cols):
        if k != i and k != j:
            sumSquares += (self.data[k][i] - self.data[k][j]) ** 2
    return math.sqrt(sumSquares)
</code></pre>

<p>得到的层次聚类结果是：</p>
<p><img alt="" src="///F://guidetodatamining/guidetodatamining/img/chapter-8/chapter-8-72.png" /></p>
<p>我还用k-means++算法进行了聚类，结果是：</p>
<p><img alt="" src="///F://guidetodatamining/guidetodatamining/img/chapter-8/chapter-8-73.png" /></p>
<p>这些结果很有趣，比如分类5中大都是贸易人员，分类7中则多是管理层。</p>
<p><img alt="" src="///F://guidetodatamining/guidetodatamining/img/chapter-8/chapter-8-74.png" /></p>
<blockquote>
<p>安然数据中还能挖掘出很多有趣的模式，去下载完整的数据集进行尝试吧！</p>
<p>你也可以对其它数据集进行聚类，看看是否有新的发现。</p>
<p>最后，恭喜完成第八章的学习！</p>
</blockquote></article></body></html>